{
    "Name": "mC4",
    "Subsets": [],
    "HF Link": "https://huggingface.co/datasets/allenai/c4",
    "Link": "https://huggingface.co/datasets/allenai/c4",
    "License": "ODC-By",
    "Year": 2019,
    "Language": "multilingual",
    "Dialect": "mixed",
    "Domain": "web pages",
    "Form": "text",
    "Collection Style": "crawling",
    "Description": "A multilingual colossal, cleaned version of Common Crawl's web crawl corpus. It contains around 108 languages. ",
    "Volume": "53,256,040",
    "Unit": "documents",
    "Ethical Risks": "High",
    "Provider": "Allen AI",
    "Derived From": "Common Crawl",
    "Paper Title": "A colossal, cleaned version of Common Crawl's web crawl corpus.",
    "Paper Link": "https://arxiv.org/pdf/1910.10683.pdf",
    "Script": "Arab",
    "Tokenized": "No",
    "Host": "HuggingFace",
    "Access": "Free",
    "Cost": "",
    "Test Split": "No",
    "Tasks": "text generation,language modeling,",
    "Venue Title": "JMLR",
    "Citations": "",
    "Venue Type": "journal",
    "Venue Name": "Journal of Machine Learning Research",
    "Authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Ktherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
    "Affiliations": "Google",
    "Abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code",
    "Added By": "Zaid Alyafeai"
}